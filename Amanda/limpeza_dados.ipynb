{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from math import ceil  # Importar a função ceil\n",
    "\n",
    "pd.options.display.max_columns=50 \n",
    "pd.options.display.max_rows=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_percent(data):\n",
    "    \"\"\"\n",
    "    Returns dataframe containing the total missing values and percentage of total\n",
    "    missing values of a column.\n",
    "    \"\"\"\n",
    "    miss_df = pd.DataFrame({'ColumnName':[],'TotalMissingVals':[],'PercentMissing':[]})\n",
    "    for col in data.columns:\n",
    "        sum_miss_val = data[col].isnull().sum()\n",
    "        percent_miss_val = round((sum_miss_val/data.shape[0])*100,2)\n",
    "        miss_df.loc[len(miss_df)] = dict(zip(miss_df.columns,[col,sum_miss_val,percent_miss_val]))\n",
    "    return miss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_outlier_percentage(series, threshold=1.5): # Function to calculate the percentage of outliers for a given series\n",
    "    z_scores = np.abs((series - series.median()) / series.std())\n",
    "    outliers = z_scores > threshold\n",
    "    return (outliers.sum() / len(series)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiple_boxplots(data_frame, columns_for_boxplot, titles=None, num_boxplots_per_row=2):\n",
    "    # Calcular a quantidade;\n",
    "    num_boxplots = len(columns_for_boxplot)\n",
    "    num_rows = (num_boxplots + num_boxplots_per_row - 1) // num_boxplots_per_row\n",
    "\n",
    "    # Criar os subplots\n",
    "    fig = make_subplots(rows=num_rows, cols=num_boxplots_per_row, subplot_titles=titles)\n",
    "\n",
    "    # Loop para ir montando todos os gráficos em boxplot\n",
    "    for idx, column in enumerate(columns_for_boxplot):\n",
    "        row_idx = idx // num_boxplots_per_row + 1\n",
    "        col_idx = idx % num_boxplots_per_row + 1\n",
    "\n",
    "        data = data_frame[column]\n",
    "        box = go.Box(y=data, name=column)\n",
    "\n",
    "        fig.add_trace(box, row=row_idx, col=col_idx)\n",
    "\n",
    "    # Ajustando a forma\n",
    "    fig.update_layout(height=300*num_rows, showlegend=False)\n",
    "\n",
    "    # Plotar os gráficos\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_distribuicoes(data_frame, columns_for_distribution, num_distributions_per_row=2):\n",
    "    # Calcular a quantidade\n",
    "    num_distributions = len(columns_for_distribution)\n",
    "    num_rows = ceil(num_distributions / num_distributions_per_row)\n",
    "\n",
    "    # Criar os subplots\n",
    "    fig = make_subplots(rows=num_rows, cols=num_distributions_per_row)\n",
    "\n",
    "    # Loop para ir montando todos os gráficos de distribuição\n",
    "    for idx, column in enumerate(columns_for_distribution):\n",
    "        dados = data_frame[column].dropna()  # Remover valores ausentes\n",
    "\n",
    "        # Criar o gráfico de histograma\n",
    "        histogram_data = go.Histogram(x=dados, nbinsx=30, name=f'Histograma - {column}')\n",
    "\n",
    "        # Adicionar ao subplot\n",
    "        fig.add_trace(histogram_data,\n",
    "                      row=(idx // num_distributions_per_row) + 1, col=(idx % num_distributions_per_row) + 1)\n",
    "\n",
    "    # Atualizar o layout com títulos e legendas adequadas\n",
    "    for idx, column in enumerate(columns_for_distribution):\n",
    "        row_idx = (idx // num_distributions_per_row) + 1\n",
    "        col_idx = (idx % num_distributions_per_row) + 1\n",
    "\n",
    "        # Adicionar título ao subplot\n",
    "        fig.update_xaxes(title_text=f'{column}', row=row_idx, col=col_idx)\n",
    "        fig.update_yaxes(title_text='Quantidade', row=row_idx, col=col_idx)  # Adicionar título ao eixo Y\n",
    "\n",
    "    # Ajustando a forma\n",
    "    fig.update_layout(height=300*num_rows, showlegend=False)  # Remover a legenda\n",
    "\n",
    "    # Plotar os gráficos\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlated_columns(df, interval):\n",
    "    \"\"\"\n",
    "    Encontra e exibe as correlações entre colunas de um DataFrame.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df: DataFrame pandas\n",
    "    - intervalo de correlação desejado (uma tupla de dois valores)\n",
    "\n",
    "    Retorna:\n",
    "    - Lista de tuplas representando pares de colunas correlacionadas.\n",
    "    \"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "    correlated_columns = []\n",
    "\n",
    "    # Iterar sobre as combinações de colunas para encontrar correlações\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            if interval[0] <= abs(corr) <= interval[1]:\n",
    "                col1 = correlation_matrix.columns[i]\n",
    "                col2 = correlation_matrix.columns[j]\n",
    "                correlated_columns.append((col1, col2))\n",
    "                print(f\"Correlação entre {col1} e {col2}: {corr}\")\n",
    "\n",
    "    # Plotar um mapa de calor da matriz de correlação\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='cubehelix_r')\n",
    "    plt.title('Matriz de Correlação')\n",
    "    plt.xlabel('Variáveis')\n",
    "    plt.ylabel('Variáveis')\n",
    "    plt.show()\n",
    "\n",
    "    return correlated_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlacao_com_variavel_alvo(df, target_variable, nivel=\"forte\", top_n=5):\n",
    "    \"\"\"\n",
    "    Imprime as n features com as maiores correlações com uma variável alvo, com base no nível escolhido.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df: DataFrame pandas.\n",
    "    - target_variable: String, nome da variável alvo.\n",
    "    - nivel: String que define o critério de correlação (\"forte\", \"fraca\", etc.).\n",
    "    - top_n: Número inteiro, quantidade de features a serem impressas.\n",
    "\n",
    "    Retorna:\n",
    "    - Nenhum (imprime as correlações).\n",
    "    \"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "    # Filtra as correlações com base no nível escolhido\n",
    "    if nivel.lower() == \"forte\":\n",
    "        filtered_correlations = correlation_matrix[((correlation_matrix >= 0.7) & (correlation_matrix < 1.0)) | ((correlation_matrix <= -0.7) & (correlation_matrix > -1.0))]\n",
    "    else:\n",
    "        raise ValueError(\"Nível não suportado. Atualmente, apenas 'forte' é suportado.\")\n",
    "\n",
    "    # Filtra as correlações com a variável alvo\n",
    "    correlations_with_target = filtered_correlations[target_variable].sort_values(ascending=False)\n",
    "\n",
    "    # Pegar as n maiores correlações\n",
    "    top_n_correlations = correlations_with_target.head(top_n)\n",
    "\n",
    "    # Imprimir as n maiores correlações com a variável alvo\n",
    "    print(f\"As {top_n} maiores correlações com '{target_variable}' ({nivel}):\")\n",
    "    for feature, correlation in top_n_correlations.items():\n",
    "        print(f\"{feature}: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise da Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos a junção de todas as tabelas em uma só para facilitar a manipulação conjunta dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando as bases\n",
    "channels = pd.read_csv('channels.csv')\n",
    "deliveries = pd.read_csv('deliveries.csv')\n",
    "drivers = pd.read_csv('drivers.csv')\n",
    "hubs = pd.read_csv('hubs.csv')\n",
    "orders = pd.read_csv('orders.csv')\n",
    "payments = pd.read_csv('payments.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "\n",
    "# Fazendo a unificação\n",
    "deliveries = pd.merge(left=drivers, right=deliveries, on='driver_id', how ='right')\n",
    "stores = pd.merge(left=hubs, right=stores, on='hub_id', how ='right')\n",
    "df = pd.merge(left=channels, right=orders, on='channel_id', how ='right')\n",
    "df = pd.merge(left=payments, right=df, on='payment_order_id', how ='right')\n",
    "df = pd.merge(left=deliveries, right=df, on='delivery_order_id', how ='right')\n",
    "df = pd.merge(left=stores, right=df, on='store_id', how ='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ColumnName</th>\n",
       "      <th>TotalMissingVals</th>\n",
       "      <th>PercentMissing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ColumnName, TotalMissingVals, PercentMissing]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Número de colunas com valores faltantes:0\n"
     ]
    }
   ],
   "source": [
    "miss_df = find_missing_percent(df)\n",
    "'''Displays columns with missing values'''\n",
    "display(miss_df[miss_df['PercentMissing']>0.0])\n",
    "print(\"\\n\")\n",
    "print(f\"Número de colunas com valores faltantes:{str(miss_df[miss_df['PercentMissing']>0.0].shape[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tirando as colunas com mais de 20% de dados faltantes\n",
    "for coluna in miss_df.loc[miss_df['PercentMissing'] > 20]['ColumnName']:\n",
    "    df.drop(columns=coluna, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com base na análise da funcionalidade de cada coluna e na porcentagem de dados faltantes, vamos focar somente nas que podem nos trazer mais resultados e excluir as mais problemáticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged = df.copy()\n",
    "\n",
    "colunas_interesse = ['hub_name', 'hub_city', 'hub_state', 'store_name', 'store_segment', 'driver_modal',\n",
    "       'driver_type', 'delivery_distance_meters', 'delivery_status',\n",
    "       'payment_amount', 'payment_fee', 'payment_method',\n",
    "       'payment_status', 'channel_name', 'channel_type',\n",
    "       'order_status', 'order_amount', 'order_delivery_fee',\n",
    "       'order_delivery_cost', 'order_created_hour', 'order_created_minute',\n",
    "       'order_created_day', 'order_created_month', 'order_created_year',\n",
    "       'order_metric_collected_time', 'order_metric_paused_time',\n",
    "       'order_metric_production_time', 'order_metric_walking_time',\n",
    "       'order_metric_expediton_speed_time', 'order_metric_transit_time',\n",
    "       'order_metric_cycle_time']\n",
    "\n",
    "'''\n",
    "mais interessantes: \n",
    "criar coluna de tempo!! ou data\n",
    "store_segment\n",
    "driver_modal\n",
    "delivery_distance_meters\n",
    "delivery_status e order_status iguais?\n",
    "payment_amount e order_amount iguais?\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "possíveis insights e análises: \n",
    "tempo médio por cidade (ou estado) -> validar com teste de hipótese se há diferença significativa\n",
    "tempo médio por driver_modal\n",
    "\n",
    "'''\n",
    "\n",
    "df = df[colunas_interesse]\n",
    "\n",
    "'''Separando dados numéricos e categóricos '''\n",
    "numeric_cols = df.select_dtypes(['float','int']).columns\n",
    "categoric_cols = df.select_dtypes('object').columns\n",
    "\n",
    "df_numeric = df[numeric_cols]\n",
    "df_categoric = df[categoric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11 % | delivery_distance_meters\n",
      "1.32 % | payment_amount\n",
      "1.86 % | payment_fee\n",
      "0.02 % | order_amount\n",
      "1.2 % | order_delivery_fee\n",
      "8.25 % | order_delivery_cost\n",
      "9.32 % | order_created_hour\n",
      "14.57 % | order_created_minute\n",
      "11.92 % | order_created_day\n",
      "19.72 % | order_created_month\n",
      "0.0 % | order_created_year\n",
      "0.43 % | order_metric_collected_time\n",
      "0.71 % | order_metric_paused_time\n",
      "0.88 % | order_metric_production_time\n",
      "0.45 % | order_metric_walking_time\n",
      "1.9 % | order_metric_expediton_speed_time\n",
      "0.26 % | order_metric_transit_time\n",
      "1.07 % | order_metric_cycle_time\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amand\\OneDrive\\PJ\\Treinamento previsao\\PJ_TREINAMENTO_3\\Amanda\\limpeza_dados.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amand/OneDrive/PJ/Treinamento%20previsao/PJ_TREINAMENTO_3/Amanda/limpeza_dados.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#Fazer boxplots, para visualizar os outliers\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amand/OneDrive/PJ/Treinamento%20previsao/PJ_TREINAMENTO_3/Amanda/limpeza_dados.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m colunas \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mdelivery_distance_meters\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpayment_amount\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpayment_fee\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amand/OneDrive/PJ/Treinamento%20previsao/PJ_TREINAMENTO_3/Amanda/limpeza_dados.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39morder_amount\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morder_delivery_fee\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morder_delivery_cost\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amand/OneDrive/PJ/Treinamento%20previsao/PJ_TREINAMENTO_3/Amanda/limpeza_dados.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39morder_created_hour\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morder_created_minute\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morder_created_day\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amand/OneDrive/PJ/Treinamento%20previsao/PJ_TREINAMENTO_3/Amanda/limpeza_dados.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39morder_metric_expediton_speed_time\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morder_metric_transit_time\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amand/OneDrive/PJ/Treinamento%20previsao/PJ_TREINAMENTO_3/Amanda/limpeza_dados.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39morder_metric_cycle_time\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/amand/OneDrive/PJ/Treinamento%20previsao/PJ_TREINAMENTO_3/Amanda/limpeza_dados.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m create_multiple_boxplots(df, colunas,num_boxplots_per_row\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\amand\\OneDrive\\PJ\\Treinamento previsao\\PJ_TREINAMENTO_3\\Amanda\\limpeza_dados.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amand/OneDrive/PJ/Treinamento%20previsao/PJ_TREINAMENTO_3/Amanda/limpeza_dados.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(height\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m\u001b[39m*\u001b[39mnum_rows, showlegend\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amand/OneDrive/PJ/Treinamento%20previsao/PJ_TREINAMENTO_3/Amanda/limpeza_dados.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Plotar os gráficos\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/amand/OneDrive/PJ/Treinamento%20previsao/PJ_TREINAMENTO_3/Amanda/limpeza_dados.ipynb#X22sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m fig\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\site-packages\\plotly\\basedatatypes.py:3398\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3365\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3366\u001b[0m \u001b[39mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[0;32m   3367\u001b[0m \u001b[39mspecified by the renderer argument\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3394\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3395\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3396\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[1;32m-> 3398\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39mshow(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\site-packages\\plotly\\io\\_renderers.py:388\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[0;32m    385\u001b[0m fig_dict \u001b[39m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n\u001b[0;32m    387\u001b[0m \u001b[39m# Mimetype renderers\u001b[39;00m\n\u001b[1;32m--> 388\u001b[0m bundle \u001b[39m=\u001b[39m renderers\u001b[39m.\u001b[39m_build_mime_bundle(fig_dict, renderers_string\u001b[39m=\u001b[39mrenderer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m bundle:\n\u001b[0;32m    390\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ipython_display:\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\site-packages\\plotly\\io\\_renderers.py:296\u001b[0m, in \u001b[0;36mRenderersConfig._build_mime_bundle\u001b[1;34m(self, fig_dict, renderers_string, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(renderer, k):\n\u001b[0;32m    294\u001b[0m                 \u001b[39msetattr\u001b[39m(renderer, k, v)\n\u001b[1;32m--> 296\u001b[0m         bundle\u001b[39m.\u001b[39mupdate(renderer\u001b[39m.\u001b[39mto_mimebundle(fig_dict))\n\u001b[0;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m bundle\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\site-packages\\plotly\\io\\_base_renderers.py:95\u001b[0m, in \u001b[0;36mPlotlyRenderer.to_mimebundle\u001b[1;34m(self, fig_dict)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mif\u001b[39;00m config:\n\u001b[0;32m     92\u001b[0m     fig_dict[\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config\n\u001b[0;32m     94\u001b[0m json_compatible_fig_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(\n\u001b[1;32m---> 95\u001b[0m     to_json(fig_dict, validate\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, remove_uids\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     96\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mapplication/vnd.plotly.v1+json\u001b[39m\u001b[39m\"\u001b[39m: json_compatible_fig_dict}\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\site-packages\\plotly\\io\\_json.py:199\u001b[0m, in \u001b[0;36mto_json\u001b[1;34m(fig, validate, pretty, remove_uids, engine)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[39mfor\u001b[39;00m trace \u001b[39min\u001b[39;00m fig_dict\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m, []):\n\u001b[0;32m    197\u001b[0m         trace\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 199\u001b[0m \u001b[39mreturn\u001b[39;00m to_json_plotly(fig_dict, pretty\u001b[39m=\u001b[39mpretty, engine\u001b[39m=\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\site-packages\\plotly\\io\\_json.py:123\u001b[0m, in \u001b[0;36mto_json_plotly\u001b[1;34m(plotly_object, pretty, engine)\u001b[0m\n\u001b[0;32m    119\u001b[0m         opts[\u001b[39m\"\u001b[39m\u001b[39mseparators\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m_plotly_utils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m PlotlyJSONEncoder\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mdumps(plotly_object, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39mPlotlyJSONEncoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopts)\n\u001b[0;32m    124\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39morjson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    125\u001b[0m     JsonConfig\u001b[39m.\u001b[39mvalidate_orjson()\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\json\\__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[0;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39mskipkeys, ensure_ascii\u001b[39m=\u001b[39mensure_ascii,\n\u001b[0;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39mcheck_circular, allow_nan\u001b[39m=\u001b[39mallow_nan, indent\u001b[39m=\u001b[39mindent,\n\u001b[0;32m    237\u001b[0m     separators\u001b[39m=\u001b[39mseparators, default\u001b[39m=\u001b[39mdefault, sort_keys\u001b[39m=\u001b[39msort_keys,\n\u001b[1;32m--> 238\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\u001b[39m.\u001b[39mencode(obj)\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\site-packages\\_plotly_utils\\utils.py:72\u001b[0m, in \u001b[0;36mPlotlyJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39m# now:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m#    1. `loads` to switch Infinity, -Infinity, NaN to None\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m#    2. `dumps` again so you get 'null' instead of extended JSON\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     new_o \u001b[39m=\u001b[39m _json\u001b[39m.\u001b[39mloads(encoded_o, parse_constant\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoerce_to_strict)\n\u001b[0;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m     \u001b[39m# invalid separators will fail here. raise a helpful exception\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     77\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEncoding into strict JSON failed. Did you set the separators \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvalid JSON separators?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\json\\__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[39mif\u001b[39;00m parse_constant \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     kw[\u001b[39m'\u001b[39m\u001b[39mparse_constant\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m parse_constant\n\u001b[1;32m--> 359\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\u001b[39m.\u001b[39mdecode(s)\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_decode(s, idx\u001b[39m=\u001b[39m_w(s, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amand\\Anaconda\\Lib\\site-packages\\_plotly_utils\\utils.py:40\u001b[0m, in \u001b[0;36mPlotlyJSONEncoder.coerce_to_strict\u001b[1;34m(self, const)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPlotlyJSONEncoder\u001b[39;00m(_json\u001b[39m.\u001b[39mJSONEncoder):\n\u001b[0;32m     29\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m    Meant to be passed as the `cls` kwarg to json.dumps(obj, cls=..)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcoerce_to_strict\u001b[39m(\u001b[39mself\u001b[39m, const):\n\u001b[0;32m     41\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m        This is used to ultimately *encode* into strict JSON, see `encode`\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         \u001b[39m# before python 2.7, 'true', 'false', 'null', were include here.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "numeric_cols = df.select_dtypes(['float','int']).columns\n",
    "for feature in numeric_cols:\n",
    "    qtd_outliers = calculate_outlier_percentage(df[feature]).round(2)\n",
    "    print(f'{qtd_outliers} % | {feature}' )\n",
    "\n",
    "#Fazer boxplots, para visualizar os outliers\n",
    "colunas = ['delivery_distance_meters', 'payment_amount', 'payment_fee',\n",
    "       'order_amount', 'order_delivery_fee', 'order_delivery_cost',\n",
    "       'order_created_hour', 'order_created_minute', 'order_created_day',\n",
    "       'order_created_month', 'order_created_year',\n",
    "       'order_metric_collected_time', 'order_metric_paused_time',\n",
    "       'order_metric_production_time', 'order_metric_walking_time',\n",
    "       'order_metric_expediton_speed_time', 'order_metric_transit_time',\n",
    "       'order_metric_cycle_time']\n",
    "create_multiple_boxplots(df, colunas,num_boxplots_per_row=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar distribuições\n",
    "plotar_distribuicoes(df, colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo outliers de delivery_distance_meters\n",
    "outliers = df[df['delivery_distance_meters'] > 7000].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de payment_amount\n",
    "outliers = df[df['payment_amount'] > 280].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de payment_fee\n",
    "outliers = df[df['payment_fee'] > 7].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_amount\n",
    "outliers = df[df['order_amount'] > 230].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_delivery_fee\n",
    "outliers = df[df['order_delivery_fee'] > 27].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_delivery_cost\n",
    "outliers = df[df['order_delivery_cost'] > 15].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_collected_time\n",
    "outliers = df[df['order_metric_collected_time'] > 6].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_paused_time\n",
    "outliers = df[ (df['order_metric_paused_time'] < 0) | (df['order_metric_paused_time'] > 12)].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_production_time\n",
    "outliers = df[df['order_metric_production_time'] > 40].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_walking_time\n",
    "outliers = df[df['order_metric_walking_time'] > 10].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_expediton_speed_time\n",
    "outliers = df[df['order_metric_expediton_speed_time'] > 20].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_transit_time\n",
    "outliers = df[df['order_metric_transit_time'] > 45].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_cycle_time\n",
    "outliers = df[df['order_metric_cycle_time'] > 90].index\n",
    "df.drop(outliers, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com base nos cálculos de outliers e visualização dos dados vamos tratar as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='order_metric_cycle_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x='order_status', y='order_metric_production_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x='driver_modal', y='order_metric_transit_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categoric.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df[colunas].describe().T,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['driver_modal', 'driver_type', 'payment_method', 'payment_status']:\n",
    "    df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "\n",
    "for column in ['delivery_distance_meters',  'payment_amount', 'payment_fee', \n",
    "                'order_delivery_cost']:\n",
    "    df[column].fillna(df[column].median(), inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Preencher NaN em 'delivery_status' com 'DELIVERED' onde 'order_metric_cycle_time' não é NaN\n",
    "df.loc[df['order_metric_cycle_time'].notnull() & df['delivery_status'].isnull(), 'delivery_status'] = 'DELIVERED'\n",
    "\n",
    "\n",
    "\n",
    "# Calcular o segundo valor mais frequente em 'delivery_status', ja que o mais frequente eh DELIVERED\n",
    "second_most_frequent_value = df['delivery_status'].value_counts().index[1]\n",
    "\n",
    "# Preencher NaN em 'delivery_status' com o segundo valor mais frequente\n",
    "df['delivery_status'].fillna(second_most_frequent_value, inplace=True)\n",
    "\n",
    "\n",
    "# Condição para preencher apenas quando delivery_status for 'DELIVERED'\n",
    "condition = df['delivery_status'] == 'DELIVERED'\n",
    "\n",
    "# Lista das colunas a serem preenchidas com a mediana\n",
    "columns_to_fill = [\n",
    "    'order_metric_production_time', \n",
    "    'order_metric_expediton_speed_time', \n",
    "    'order_metric_transit_time', \n",
    "    'order_metric_collected_time', \n",
    "    'order_metric_paused_time', \n",
    "    'order_metric_walking_time'\n",
    "]\n",
    "\n",
    "# Preencher os valores faltantes com a mediana nas colunas específicas quando delivery_status for 'DELIVERED'\n",
    "for column in columns_to_fill:\n",
    "    median_value = df.loc[condition, column].median()\n",
    "    df.loc[condition & df[column].isnull(), column] = median_value\n",
    "\n",
    "\n",
    "\n",
    "# Criar a soma das colunas desejadas\n",
    "sum_columns = df[\n",
    "    ['order_metric_production_time', \n",
    "    'order_metric_expediton_speed_time', \n",
    "    'order_metric_transit_time', \n",
    "    'order_metric_collected_time', \n",
    "    'order_metric_paused_time', \n",
    "    'order_metric_walking_time']\n",
    "].sum(axis=1)\n",
    "\n",
    "# Preencher os valores faltantes em 'order_metric_cycle_time' com a soma das colunas\n",
    "df['order_metric_cycle_time'].fillna(sum_columns, inplace=True)\n",
    "\n",
    "\n",
    "# Descartar linhas onde a condição não é satisfeita\n",
    "df = df[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
