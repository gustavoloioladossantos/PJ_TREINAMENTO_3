{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from math import ceil  # Importar a função ceil\n",
    "\n",
    "pd.options.display.max_columns=70 \n",
    "pd.options.display.max_rows=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_percent(data):\n",
    "    \"\"\"\n",
    "    Retorna dataframe contendo o total de valores faltantes e porcentagem do total\n",
    "    de valores faltantes da coluna.\n",
    "    \"\"\"\n",
    "    miss_df = pd.DataFrame({'ColumnName':[],'TotalMissingVals':[],'PercentMissing':[]})\n",
    "    for col in data.columns:\n",
    "        sum_miss_val = data[col].isnull().sum()\n",
    "        percent_miss_val = round((sum_miss_val/data.shape[0])*100,2)\n",
    "        miss_df.loc[len(miss_df)] = dict(zip(miss_df.columns,[col,sum_miss_val,percent_miss_val]))\n",
    "    return miss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_outlier_percentage(series, threshold=1.5): # Function to calculate the percentage of outliers for a given series\n",
    "    z_scores = np.abs((series - series.median()) / series.std())\n",
    "    outliers = z_scores > threshold\n",
    "    return (outliers.sum() / len(series)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiple_boxplots(data_frame, columns_for_boxplot, titles=None, num_boxplots_per_row=2):\n",
    "    # Calcular a quantidade;\n",
    "    num_boxplots = len(columns_for_boxplot)\n",
    "    num_rows = (num_boxplots + num_boxplots_per_row - 1) // num_boxplots_per_row\n",
    "\n",
    "    # Criar os subplots\n",
    "    fig = make_subplots(rows=num_rows, cols=num_boxplots_per_row, subplot_titles=titles)\n",
    "\n",
    "    # Loop para ir montando todos os gráficos em boxplot\n",
    "    for idx, column in enumerate(columns_for_boxplot):\n",
    "        row_idx = idx // num_boxplots_per_row + 1\n",
    "        col_idx = idx % num_boxplots_per_row + 1\n",
    "\n",
    "        data = data_frame[column]\n",
    "        box = go.Box(y=data, name=column)\n",
    "\n",
    "        fig.add_trace(box, row=row_idx, col=col_idx)\n",
    "\n",
    "    # Ajustando a forma\n",
    "    fig.update_layout(height=300*num_rows, showlegend=False)\n",
    "\n",
    "    # Plotar os gráficos\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_distribuicoes(data_frame, columns_for_distribution, num_distributions_per_row=2):\n",
    "    # Calcular a quantidade\n",
    "    num_distributions = len(columns_for_distribution)\n",
    "    num_rows = ceil(num_distributions / num_distributions_per_row)\n",
    "\n",
    "    # Criar os subplots\n",
    "    fig = make_subplots(rows=num_rows, cols=num_distributions_per_row)\n",
    "\n",
    "    # Loop para ir montando todos os gráficos de distribuição\n",
    "    for idx, column in enumerate(columns_for_distribution):\n",
    "        dados = data_frame[column].dropna()  # Remover valores ausentes\n",
    "\n",
    "        # Criar o gráfico de histograma\n",
    "        histogram_data = go.Histogram(x=dados, nbinsx=30, name=f'Histograma - {column}')\n",
    "\n",
    "        # Adicionar ao subplot\n",
    "        fig.add_trace(histogram_data,\n",
    "                      row=(idx // num_distributions_per_row) + 1, col=(idx % num_distributions_per_row) + 1)\n",
    "\n",
    "    # Atualizar o layout com títulos e legendas adequadas\n",
    "    for idx, column in enumerate(columns_for_distribution):\n",
    "        row_idx = (idx // num_distributions_per_row) + 1\n",
    "        col_idx = (idx % num_distributions_per_row) + 1\n",
    "\n",
    "        # Adicionar título ao subplot\n",
    "        fig.update_xaxes(title_text=f'{column}', row=row_idx, col=col_idx)\n",
    "        fig.update_yaxes(title_text='Quantidade', row=row_idx, col=col_idx)  # Adicionar título ao eixo Y\n",
    "\n",
    "    # Ajustando a forma\n",
    "    fig.update_layout(height=300*num_rows, showlegend=False)  # Remover a legenda\n",
    "\n",
    "    # Plotar os gráficos\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlated_columns(df, interval):\n",
    "    \"\"\"\n",
    "    Encontra e exibe as correlações entre colunas de um DataFrame.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df: DataFrame pandas\n",
    "    - intervalo de correlação desejado (uma tupla de dois valores)\n",
    "\n",
    "    Retorna:\n",
    "    - Lista de tuplas representando pares de colunas correlacionadas.\n",
    "    \"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "    correlated_columns = []\n",
    "\n",
    "    # Iterar sobre as combinações de colunas para encontrar correlações\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            if interval[0] <= abs(corr) <= interval[1]:\n",
    "                col1 = correlation_matrix.columns[i]\n",
    "                col2 = correlation_matrix.columns[j]\n",
    "                correlated_columns.append((col1, col2))\n",
    "                print(f\"Correlação entre {col1} e {col2}: {corr}\")\n",
    "\n",
    "    # Plotar um mapa de calor da matriz de correlação\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='cubehelix_r')\n",
    "    plt.title('Matriz de Correlação')\n",
    "    plt.xlabel('Variáveis')\n",
    "    plt.ylabel('Variáveis')\n",
    "    plt.show()\n",
    "\n",
    "    return correlated_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlacao_com_variavel_alvo(df, target_variable, nivel=\"forte\", top_n=5):\n",
    "    \"\"\"\n",
    "    Imprime as n features com as maiores correlações com uma variável alvo, com base no nível escolhido.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df: DataFrame pandas.\n",
    "    - target_variable: String, nome da variável alvo.\n",
    "    - nivel: String que define o critério de correlação (\"forte\", \"fraca\", etc.).\n",
    "    - top_n: Número inteiro, quantidade de features a serem impressas.\n",
    "\n",
    "    Retorna:\n",
    "    - Nenhum (imprime as correlações).\n",
    "    \"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "    # Filtra as correlações com base no nível escolhido\n",
    "    if nivel.lower() == \"forte\":\n",
    "        filtered_correlations = correlation_matrix[((correlation_matrix >= 0.7) & (correlation_matrix < 1.0)) | ((correlation_matrix <= -0.7) & (correlation_matrix > -1.0))]\n",
    "    else:\n",
    "        raise ValueError(\"Nível não suportado. Atualmente, apenas 'forte' é suportado.\")\n",
    "\n",
    "    # Filtra as correlações com a variável alvo\n",
    "    correlations_with_target = filtered_correlations[target_variable].sort_values(ascending=False)\n",
    "\n",
    "    # Pegar as n maiores correlações\n",
    "    top_n_correlations = correlations_with_target.head(top_n)\n",
    "\n",
    "    # Imprimir as n maiores correlações com a variável alvo\n",
    "    print(f\"As {top_n} maiores correlações com '{target_variable}' ({nivel}):\")\n",
    "    for feature, correlation in top_n_correlations.items():\n",
    "        print(f\"{feature}: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_multiple_col(col_names_list, df): \n",
    "    '''\n",
    "    AIM    -> Drop multiple columns based on their column names \n",
    "    \n",
    "    INPUT  -> List of column names, df\n",
    "    \n",
    "    OUTPUT -> updated df with dropped columns \n",
    "    ------\n",
    "    '''\n",
    "    df.drop(col_names_list, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format_to_datetime(dataframe, columns):\n",
    "    \"\"\"\n",
    "    Convert specified columns in a DataFrame from 'DD/MM/YYYY HH:MM:SS PM' to 'YYYY-MM-DD HH:MM:SS' format.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): The DataFrame to modify.\n",
    "    - columns (list): List of column names to convert.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        if column in dataframe.columns:\n",
    "            # Convert the specified column to datetime with the original format\n",
    "            dataframe[column] = pd.to_datetime(dataframe[column], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n",
    "        else:\n",
    "            print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise da Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos a junção de todas as tabelas em uma só para facilitar a manipulação conjunta dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando as bases\n",
    "channels = pd.read_csv('channels.csv')\n",
    "deliveries = pd.read_csv('deliveries.csv')\n",
    "drivers = pd.read_csv('drivers.csv')\n",
    "hubs = pd.read_csv('hubs.csv')\n",
    "orders = pd.read_csv('orders.csv')\n",
    "payments = pd.read_csv('payments.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "\n",
    "# Fazendo a unificação\n",
    "deliveries = pd.merge(left=drivers, right=deliveries, on='driver_id', how ='right')\n",
    "stores = pd.merge(left=hubs, right=stores, on='hub_id', how ='right')\n",
    "df = pd.merge(left=channels, right=orders, on='channel_id', how ='right')\n",
    "df = pd.merge(left=payments, right=df, on='payment_order_id', how ='right')\n",
    "df = pd.merge(left=deliveries, right=df, on='delivery_order_id', how ='right')\n",
    "df = pd.merge(left=stores, right=df, on='store_id', how ='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moment_order_columns = ['order_moment_created', 'order_moment_accepted','order_moment_ready', \n",
    "                        'order_moment_collected','order_moment_in_expedition', 'order_moment_delivering',\n",
    "                        'order_moment_finished']\n",
    "\n",
    "# Convert specified columns to datetime\n",
    "df = convert_format_to_datetime(df, columns=moment_order_columns)\n",
    "\n",
    "# Criando uma coluna com o tempo de entrega do pedido (essa é a variável que vai prevista) - se assemelha MUITO com order_metric_cycle_time\n",
    "df['tempo_entrega'] = (df['order_moment_finished'] - df['order_moment_accepted']).dt.round('MIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma coluna de data com base nas colunas existentes\n",
    "df.rename(columns = {'order_created_year':'year','order_created_month':'month','order_created_day':'day', 'order_created_hour': 'hour', 'order_created_minute': 'minute'}, inplace=True)\n",
    "df['order_date'] = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df = find_missing_percent(df)\n",
    "'''Displays columns with missing values'''\n",
    "display(miss_df[miss_df['PercentMissing']>0.0])\n",
    "print(\"\\n\")\n",
    "print(f\"Número de colunas com valores faltantes:{str(miss_df[miss_df['PercentMissing']>0.0].shape[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tirando as colunas com mais de 70% de dados faltantes\n",
    "for coluna in miss_df.loc[miss_df['PercentMissing'] > 70]['ColumnName']:\n",
    "    df.drop(columns=coluna, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com base na análise da funcionalidade de cada coluna e na porcentagem de dados faltantes, vamos focar somente nas que podem nos trazer mais resultados e excluir as mais problemáticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observação: A coluna criada 'tempo_entrega' que utiliza o 'order_moment_accepted' e 'order_moment_finished' apresenta MUITA SEMELHANÇA com a coluna 'order_metric_cycle_time'.\n",
    "\n",
    "Quando os valores são muito diferentes, isso se dá porque o 'order_moment_finished', que corresponde ao momento que o estabelecimento finalizou o pedido, está muito depois do tempo correspondente à entrega em si, o que denota uma inconsistência nos dados\n",
    "\n",
    "Devido à isso, a coluna 'tempo_entrega' será desconsiderada para futuras análises e a métrica utilizada pra fazer as previsões em etapas posteriores será 'order_metric_cycle_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_descart = ['hub_id', 'hub_latitude', 'hub_longitude', 'store_id', 'store_latitude', 'store_longitude', 'driver_id',\n",
    "                   'delivery_id', 'delivery_order_id','payment_id', 'payment_order_id', 'channel_id', 'order_id',\n",
    "                   'hour', 'minute', 'day', 'month', 'year', 'tempo_entrega']\n",
    "#metric_cycle_columns = ['order_metric_collected_time', 'order_metric_paused_time',\n",
    "#       'order_metric_production_time', 'order_metric_walking_time',\n",
    "#       'order_metric_expediton_speed_time', 'order_metric_transit_time',\n",
    "#       'order_metric_cycle_time']\n",
    "#colunas_descart = colunas_descart + metric_cycle_columns\n",
    "\n",
    "df = drop_multiple_col(colunas_descart, df)\n",
    "\n",
    "'''Separando dados numéricos e categóricos '''\n",
    "numeric_cols = df.select_dtypes(['float','int']).columns\n",
    "categoric_cols = df.select_dtypes('object').columns\n",
    "\n",
    "df_numeric = df[numeric_cols]\n",
    "df_categoric = df[categoric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df = find_missing_percent(df)\n",
    "'''Displays columns with missing values'''\n",
    "display(miss_df[miss_df['PercentMissing']>0.0])\n",
    "print(\"\\n\")\n",
    "print(f\"Número de colunas com valores faltantes:{str(miss_df[miss_df['PercentMissing']>0.0].shape[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(['float','int']).columns\n",
    "for feature in numeric_cols:\n",
    "    qtd_outliers = calculate_outlier_percentage(df[feature]).round(2)\n",
    "    print(f'{qtd_outliers} % | {feature}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazer boxplots, para visualizar os outliers\n",
    "#create_multiple_boxplots(df, df_numeric.columns,num_boxplots_per_row=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar distribuições\n",
    "#plotar_distribuicoes(df, df_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_antes = df.shape[0]\n",
    "\n",
    "# removendo outliers de delivery_distance_meters\n",
    "outliers = df[df['delivery_distance_meters'] > 10000].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de payment_amount\n",
    "outliers = df[df['payment_amount'] > 300].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de payment_fee\n",
    "outliers = df[df['payment_fee'] > 10].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_amount\n",
    "outliers = df[df['order_amount'] > 300].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_delivery_fee\n",
    "outliers = df[df['order_delivery_fee'] > 27].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_delivery_cost\n",
    "outliers = df[df['order_delivery_cost'] > 15].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_collected_time\n",
    "outliers = df[(df['order_metric_collected_time'] > 7) | (df['order_metric_walking_time'] < 0)].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_paused_time\n",
    "outliers = df[ (df['order_metric_paused_time'] < 0) | (df['order_metric_paused_time'] > 20)].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_production_time\n",
    "outliers = df[df['order_metric_production_time'] > 45].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_walking_time\n",
    "outliers = df[(df['order_metric_walking_time'] > 10) | (df['order_metric_walking_time'] < 0)].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_expediton_speed_time\n",
    "outliers = df[df['order_metric_expediton_speed_time'] > 20].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_transit_time\n",
    "outliers = df[df['order_metric_transit_time'] > 45].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_cycle_time\n",
    "outliers = df[df['order_metric_cycle_time'] > 90].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de tempo_entrega que excedam 1 dia para realizar a entrega\n",
    "#outliers = df[df['tempo_entrega'] > pd.Timedelta(days=1)].index\n",
    "#df.drop(outliers, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "linhas_depois = df.shape[0]\n",
    "restante = round((100* linhas_depois / linhas_antes), 2)\n",
    "print(f'total de linhas retiradas: {linhas_antes - linhas_depois} (restam {linhas_depois} linhas que equivalem a {restante}% da base inicial)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['payment_status'].eq('AWAITING')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo AWAITING de 'paymen_status' porque são somente 10 valores e 8 deles não apresentam a métrica de interesse \n",
    "outliers = df[df['payment_status'].eq('AWAITING')].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar distribuições\n",
    "plotar_distribuicoes(df, df_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_verificar = ['order_metric_cycle_time','order_metric_transit_time', 'order_metric_expediton_speed_time', \n",
    "                     'order_metric_walking_time', 'order_metric_production_time', 'order_metric_collected_time']\n",
    "for coluna in colunas_verificar:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.histplot(df, x=coluna)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['order_metric_cycle_time'].eq(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df.describe().T,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportando arquivo para utilizar nas etapas posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtd_linhas = len(df)\n",
    "\n",
    "print(qtd_linhas)\n",
    "print(df.loc[:(len(df)/2)].shape)\n",
    "print(df.loc[(len(df)/2):].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo em 2 arquivos por conta do tamanho\n",
    "\n",
    "df1 = df.loc[:(len(df)/2)]\n",
    "df2 = df.loc[(len(df)/2):]\n",
    "\n",
    "df1.to_csv('base_limpa1.csv')\n",
    "df2.to_csv('base_limpa2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
