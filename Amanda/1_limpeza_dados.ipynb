{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from math import ceil  # Importar a função ceil\n",
    "\n",
    "pd.options.display.max_columns=50 \n",
    "pd.options.display.max_rows=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_percent(data):\n",
    "    \"\"\"\n",
    "    Retorna dataframe contendo o total de valores faltantes e porcentagem do total\n",
    "    de valores faltantes da coluna.\n",
    "    \"\"\"\n",
    "    miss_df = pd.DataFrame({'ColumnName':[],'TotalMissingVals':[],'PercentMissing':[]})\n",
    "    for col in data.columns:\n",
    "        sum_miss_val = data[col].isnull().sum()\n",
    "        percent_miss_val = round((sum_miss_val/data.shape[0])*100,2)\n",
    "        miss_df.loc[len(miss_df)] = dict(zip(miss_df.columns,[col,sum_miss_val,percent_miss_val]))\n",
    "    return miss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_outlier_percentage(series, threshold=1.5): # Function to calculate the percentage of outliers for a given series\n",
    "    z_scores = np.abs((series - series.median()) / series.std())\n",
    "    outliers = z_scores > threshold\n",
    "    return (outliers.sum() / len(series)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiple_boxplots(data_frame, columns_for_boxplot, titles=None, num_boxplots_per_row=2):\n",
    "    # Calcular a quantidade;\n",
    "    num_boxplots = len(columns_for_boxplot)\n",
    "    num_rows = (num_boxplots + num_boxplots_per_row - 1) // num_boxplots_per_row\n",
    "\n",
    "    # Criar os subplots\n",
    "    fig = make_subplots(rows=num_rows, cols=num_boxplots_per_row, subplot_titles=titles)\n",
    "\n",
    "    # Loop para ir montando todos os gráficos em boxplot\n",
    "    for idx, column in enumerate(columns_for_boxplot):\n",
    "        row_idx = idx // num_boxplots_per_row + 1\n",
    "        col_idx = idx % num_boxplots_per_row + 1\n",
    "\n",
    "        data = data_frame[column]\n",
    "        box = go.Box(y=data, name=column)\n",
    "\n",
    "        fig.add_trace(box, row=row_idx, col=col_idx)\n",
    "\n",
    "    # Ajustando a forma\n",
    "    fig.update_layout(height=300*num_rows, showlegend=False)\n",
    "\n",
    "    # Plotar os gráficos\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_distribuicoes(data_frame, columns_for_distribution, num_distributions_per_row=2):\n",
    "    # Calcular a quantidade\n",
    "    num_distributions = len(columns_for_distribution)\n",
    "    num_rows = ceil(num_distributions / num_distributions_per_row)\n",
    "\n",
    "    # Criar os subplots\n",
    "    fig = make_subplots(rows=num_rows, cols=num_distributions_per_row)\n",
    "\n",
    "    # Loop para ir montando todos os gráficos de distribuição\n",
    "    for idx, column in enumerate(columns_for_distribution):\n",
    "        dados = data_frame[column].dropna()  # Remover valores ausentes\n",
    "\n",
    "        # Criar o gráfico de histograma\n",
    "        histogram_data = go.Histogram(x=dados, nbinsx=30, name=f'Histograma - {column}')\n",
    "\n",
    "        # Adicionar ao subplot\n",
    "        fig.add_trace(histogram_data,\n",
    "                      row=(idx // num_distributions_per_row) + 1, col=(idx % num_distributions_per_row) + 1)\n",
    "\n",
    "    # Atualizar o layout com títulos e legendas adequadas\n",
    "    for idx, column in enumerate(columns_for_distribution):\n",
    "        row_idx = (idx // num_distributions_per_row) + 1\n",
    "        col_idx = (idx % num_distributions_per_row) + 1\n",
    "\n",
    "        # Adicionar título ao subplot\n",
    "        fig.update_xaxes(title_text=f'{column}', row=row_idx, col=col_idx)\n",
    "        fig.update_yaxes(title_text='Quantidade', row=row_idx, col=col_idx)  # Adicionar título ao eixo Y\n",
    "\n",
    "    # Ajustando a forma\n",
    "    fig.update_layout(height=300*num_rows, showlegend=False)  # Remover a legenda\n",
    "\n",
    "    # Plotar os gráficos\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlated_columns(df, interval):\n",
    "    \"\"\"\n",
    "    Encontra e exibe as correlações entre colunas de um DataFrame.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df: DataFrame pandas\n",
    "    - intervalo de correlação desejado (uma tupla de dois valores)\n",
    "\n",
    "    Retorna:\n",
    "    - Lista de tuplas representando pares de colunas correlacionadas.\n",
    "    \"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "    correlated_columns = []\n",
    "\n",
    "    # Iterar sobre as combinações de colunas para encontrar correlações\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            if interval[0] <= abs(corr) <= interval[1]:\n",
    "                col1 = correlation_matrix.columns[i]\n",
    "                col2 = correlation_matrix.columns[j]\n",
    "                correlated_columns.append((col1, col2))\n",
    "                print(f\"Correlação entre {col1} e {col2}: {corr}\")\n",
    "\n",
    "    # Plotar um mapa de calor da matriz de correlação\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='cubehelix_r')\n",
    "    plt.title('Matriz de Correlação')\n",
    "    plt.xlabel('Variáveis')\n",
    "    plt.ylabel('Variáveis')\n",
    "    plt.show()\n",
    "\n",
    "    return correlated_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlacao_com_variavel_alvo(df, target_variable, nivel=\"forte\", top_n=5):\n",
    "    \"\"\"\n",
    "    Imprime as n features com as maiores correlações com uma variável alvo, com base no nível escolhido.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df: DataFrame pandas.\n",
    "    - target_variable: String, nome da variável alvo.\n",
    "    - nivel: String que define o critério de correlação (\"forte\", \"fraca\", etc.).\n",
    "    - top_n: Número inteiro, quantidade de features a serem impressas.\n",
    "\n",
    "    Retorna:\n",
    "    - Nenhum (imprime as correlações).\n",
    "    \"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "    # Filtra as correlações com base no nível escolhido\n",
    "    if nivel.lower() == \"forte\":\n",
    "        filtered_correlations = correlation_matrix[((correlation_matrix >= 0.7) & (correlation_matrix < 1.0)) | ((correlation_matrix <= -0.7) & (correlation_matrix > -1.0))]\n",
    "    else:\n",
    "        raise ValueError(\"Nível não suportado. Atualmente, apenas 'forte' é suportado.\")\n",
    "\n",
    "    # Filtra as correlações com a variável alvo\n",
    "    correlations_with_target = filtered_correlations[target_variable].sort_values(ascending=False)\n",
    "\n",
    "    # Pegar as n maiores correlações\n",
    "    top_n_correlations = correlations_with_target.head(top_n)\n",
    "\n",
    "    # Imprimir as n maiores correlações com a variável alvo\n",
    "    print(f\"As {top_n} maiores correlações com '{target_variable}' ({nivel}):\")\n",
    "    for feature, correlation in top_n_correlations.items():\n",
    "        print(f\"{feature}: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise da Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos a junção de todas as tabelas em uma só para facilitar a manipulação conjunta dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando as bases\n",
    "channels = pd.read_csv('channels.csv')\n",
    "deliveries = pd.read_csv('deliveries.csv')\n",
    "drivers = pd.read_csv('drivers.csv')\n",
    "hubs = pd.read_csv('hubs.csv')\n",
    "orders = pd.read_csv('orders.csv')\n",
    "payments = pd.read_csv('payments.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "\n",
    "# Fazendo a unificação\n",
    "deliveries = pd.merge(left=drivers, right=deliveries, on='driver_id', how ='right')\n",
    "stores = pd.merge(left=hubs, right=stores, on='hub_id', how ='right')\n",
    "df = pd.merge(left=channels, right=orders, on='channel_id', how ='right')\n",
    "df = pd.merge(left=payments, right=df, on='payment_order_id', how ='right')\n",
    "df = pd.merge(left=deliveries, right=df, on='delivery_order_id', how ='right')\n",
    "df = pd.merge(left=stores, right=df, on='store_id', how ='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma coluna de data com base nas colunas existentes\n",
    "df.rename(columns = {'order_created_year':'year','order_created_month':'month','order_created_day':'day', 'order_created_hour': 'hour', 'order_created_minute': 'minute'}, inplace=True)\n",
    "df['order_date'] = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute']])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df = find_missing_percent(df)\n",
    "'''Displays columns with missing values'''\n",
    "display(miss_df[miss_df['PercentMissing']>0.0])\n",
    "print(\"\\n\")\n",
    "print(f\"Número de colunas com valores faltantes:{str(miss_df[miss_df['PercentMissing']>0.0].shape[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tirando as colunas com mais de 20% de dados faltantes\n",
    "for coluna in miss_df.loc[miss_df['PercentMissing'] > 20]['ColumnName']:\n",
    "    df.drop(columns=coluna, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com base na análise da funcionalidade de cada coluna e na porcentagem de dados faltantes, vamos focar somente nas que podem nos trazer mais resultados e excluir as mais problemáticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged = df.copy()\n",
    "\n",
    "colunas_interesse = ['hub_name', 'hub_city', 'hub_state',\n",
    "       'store_name', 'store_segment',\n",
    "       'driver_modal',\n",
    "       'driver_type', 'delivery_distance_meters', 'delivery_status',\n",
    "       'payment_amount', 'payment_fee', 'payment_method',\n",
    "       'payment_status', 'channel_name', 'channel_type',\n",
    "       'order_status', 'order_amount', 'order_delivery_fee',\n",
    "       'order_delivery_cost',\n",
    "       'order_metric_collected_time', 'order_metric_paused_time',\n",
    "       'order_metric_production_time', 'order_metric_walking_time',\n",
    "       'order_metric_expediton_speed_time', 'order_metric_transit_time',\n",
    "       'order_metric_cycle_time', 'order_date']\n",
    "\n",
    "'''\n",
    "mais interessantes: \n",
    "criar coluna de tempo!! ou data\n",
    "store_segment\n",
    "driver_modal\n",
    "delivery_distance_meters\n",
    "delivery_status e order_status iguais?\n",
    "payment_amount e order_amount iguais?\n",
    "'''\n",
    "\n",
    "df = df[colunas_interesse]\n",
    "\n",
    "'''Separando dados numéricos e categóricos '''\n",
    "numeric_cols = df.select_dtypes(['float','int']).columns\n",
    "categoric_cols = df.select_dtypes('object').columns\n",
    "\n",
    "df_numeric = df[numeric_cols]\n",
    "df_categoric = df[categoric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(['float','int']).columns\n",
    "for feature in numeric_cols:\n",
    "    qtd_outliers = calculate_outlier_percentage(df[feature]).round(2)\n",
    "    print(f'{qtd_outliers} % | {feature}' )\n",
    "\n",
    "#Fazer boxplots, para visualizar os outliers\n",
    "colunas = ['delivery_distance_meters', 'payment_amount', 'payment_fee',\n",
    "       'order_amount', 'order_delivery_fee', 'order_delivery_cost',\n",
    "       'order_metric_collected_time', 'order_metric_paused_time',\n",
    "       'order_metric_production_time', 'order_metric_walking_time',\n",
    "       'order_metric_expediton_speed_time', 'order_metric_transit_time',\n",
    "       'order_metric_cycle_time']\n",
    "#create_multiple_boxplots(df, colunas,num_boxplots_per_row=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar distribuições\n",
    "#plotar_distribuicoes(df, colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_antes = df.shape[0]\n",
    "\n",
    "# removendo outliers de delivery_distance_meters\n",
    "outliers = df[df['delivery_distance_meters'] > 7000].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de payment_amount\n",
    "outliers = df[df['payment_amount'] > 280].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de payment_fee\n",
    "outliers = df[df['payment_fee'] > 10].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_amount\n",
    "outliers = df[df['order_amount'] > 280].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_delivery_fee\n",
    "outliers = df[df['order_delivery_fee'] > 27].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_delivery_cost\n",
    "outliers = df[df['order_delivery_cost'] > 15].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_collected_time\n",
    "outliers = df[(df['order_metric_collected_time'] > 7) | (df['order_metric_walking_time'] < 0)].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_paused_time\n",
    "outliers = df[ (df['order_metric_paused_time'] < 0) | (df['order_metric_paused_time'] > 20)].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_production_time\n",
    "outliers = df[df['order_metric_production_time'] > 45].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_walking_time\n",
    "outliers = df[(df['order_metric_walking_time'] > 10) | (df['order_metric_walking_time'] < 0)].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_expediton_speed_time\n",
    "outliers = df[df['order_metric_expediton_speed_time'] > 20].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_transit_time\n",
    "outliers = df[df['order_metric_transit_time'] > 45].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_cycle_time\n",
    "outliers = df[df['order_metric_cycle_time'] > 80].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "linhas_depois = df.shape[0]\n",
    "restante = round((100* linhas_depois / linhas_antes), 2)\n",
    "print(f'total de linhas retiradas: {linhas_antes - linhas_depois} (restam {linhas_depois} linhas que equivalem a {restante}% da base inicial)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_verificar = ['order_metric_cycle_time','order_metric_transit_time', 'order_metric_expediton_speed_time', \n",
    "                     'order_metric_walking_time', 'order_metric_production_time', 'order_metric_collected_time']\n",
    "for coluna in colunas_verificar:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.histplot(df, x=coluna)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['order_metric_cycle_time'].eq(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df[colunas].describe().T,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com base nos cálculos de outliers e visualização dos dados vamos tratar as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.histplot(df, x='order_metric_cycle_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(data=df, x='order_status', y='order_metric_production_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(data=df, x='driver_modal', y='order_metric_transit_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_numeric.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categoric.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df[colunas].describe().T,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preenchendo vazios #\n",
    "\n",
    "Quando se tem muito outlier, a mediana eh utilizada, como removemos os outliers nos codigos acima, nao utilizaremos a mediana.\n",
    "Todos os dados que vamos substituir seguem uma distribuicao discreta, o que convem substituir pela moda. Mas para os dados com distribuicao normal, eh possivel de se substituir pela media, dado que os outliers nao interfiram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preencher NaN em 'delivery_status' com 'DELIVERED' onde 'order_metric_cycle_time' não é NaN, pois sempre que tem o tempo do ciclo total, eh pq ele foi entregue\n",
    "df.loc[df['order_metric_cycle_time'].notnull() & df['delivery_status'].isnull(), 'delivery_status'] = 'DELIVERED'\n",
    "\n",
    "\n",
    "# Calcular o segundo valor mais frequente em 'delivery_status', ja que o mais frequente eh DELIVERED\n",
    "second_most_frequent_value = df['delivery_status'].value_counts().index[1]\n",
    "\n",
    "# Preencher NaN em 'delivery_status' com o segundo valor mais frequente, pois sabemos que nao foi entregue, agora ele pode estar sendo entregue ou pode ter sido cancelado\n",
    "df['delivery_status'].fillna(second_most_frequent_value, inplace=True)\n",
    "\n",
    "# Essas duas colunas sempre devem estar preenchidas, independentemente do status de entrega\n",
    "df['delivery_distance_meters'].fillna(df['delivery_distance_meters'].mean(), inplace=True)\n",
    "df['order_delivery_cost'].fillna(df['order_delivery_cost'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "#A seguir colunas que so possuem valor se o pedido nao foi cancelado\n",
    "# Condição para preencher apenas quando delivery_status nao for 'CANCELED'\n",
    "condition1 = df['delivery_status'] =! 'CANCELED'\n",
    "\n",
    "# Lista das colunas a serem preenchidas com a moda\n",
    "columns_to_fill_mode = [\n",
    "    'driver_modal',\n",
    "    'driver_type',\n",
    "    'payment_method',\n",
    "    'payment_status',\n",
    "    'payment_amount',\n",
    "    'payment_fee'\n",
    "]\n",
    "\n",
    "# Preencher os valores faltantes com a mediana nas colunas específicas quando delivery_status for 'DELIVERED'\n",
    "for column in columns_to_fill_mode:\n",
    "    mode_value = df.loc[condition1, column].mean()\n",
    "    df.loc[condition & df[column].isnull(), column] = mode_value\n",
    "\n",
    "\n",
    "#A seguir colunas que so possuem valor se o pedido foi entregue\n",
    "# Condição para preencher apenas quando delivery_status for 'DELIVERED'\n",
    "condition2 = df['delivery_status'] == 'DELIVERED'\n",
    "\n",
    "# Lista das colunas a serem preenchidas com a media, pois seguem uma distribuicao normal\n",
    "columns_to_fill_mean = [\n",
    "    'order_metric_production_time', \n",
    "    'order_metric_expediton_speed_time', \n",
    "    'order_metric_transit_time', \n",
    "    'order_metric_walking_time'\n",
    "]\n",
    "\n",
    "# Preencher os valores faltantes com a mediana nas colunas específicas quando delivery_status for 'DELIVERED'\n",
    "for column in columns_to_fill_mean:\n",
    "    mean_value = df.loc[condition2, column].mean()\n",
    "    df.loc[condition & df[column].isnull(), column] = mean_value\n",
    "\n",
    "    \n",
    "# Lista das colunas a serem preenchidas com a moda, pois nao seguem uma distribuicao normal\n",
    "columns_to_fill_mode = [\n",
    "    'order_metric_collected_time', \n",
    "    'order_metric_paused_time'\n",
    "]\n",
    "\n",
    "# Preencher os valores faltantes com a media nas colunas específicas quando delivery_status for 'DELIVERED'\n",
    "for column in columns_to_fill_mode:\n",
    "    mode_value = df.loc[condition2, column].mean()\n",
    "    df.loc[condition & df[column].isnull(), column] = mode_value\n",
    "\n",
    "\n",
    "\n",
    "# Criar a soma das colunas desejadas\n",
    "sum_columns = df[\n",
    "    ['order_metric_production_time', \n",
    "    'order_metric_expediton_speed_time', \n",
    "    'order_metric_transit_time', \n",
    "    'order_metric_collected_time', \n",
    "    'order_metric_paused_time', \n",
    "    'order_metric_walking_time']\n",
    "].sum(axis=1)\n",
    "\n",
    "# Preencher os valores faltantes em 'order_metric_cycle_time' com a soma das colunas\n",
    "df['order_metric_cycle_time'].fillna(sum_columns, inplace=True)\n",
    "\n",
    "\n",
    "# Descartar linhas onde a condição não é satisfeita\n",
    "df = df[condition]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Merged')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
