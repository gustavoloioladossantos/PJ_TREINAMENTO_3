{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from math import ceil \n",
    "from pprint import pprint\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "pd.options.display.max_columns=100 \n",
    "pd.options.display.max_rows=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_percent(data):\n",
    "    \"\"\"\n",
    "    Retorna dataframe contendo o total de valores faltantes e porcentagem do total\n",
    "    de valores faltantes da coluna.\n",
    "    \"\"\"\n",
    "    miss_df = pd.DataFrame({'ColumnName':[],'TotalMissingVals':[],'PercentMissing':[]})\n",
    "    for col in data.columns:\n",
    "        sum_miss_val = data[col].isnull().sum()\n",
    "        percent_miss_val = round((sum_miss_val/data.shape[0])*100,2)\n",
    "        miss_df.loc[len(miss_df)] = dict(zip(miss_df.columns,[col,sum_miss_val,percent_miss_val]))\n",
    "    return miss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_outlier_percentage(series, threshold=1.5):\n",
    "     \"\"\"\n",
    "     Função para calcular porcentagem de outliers para uma dada série\n",
    "     \"\"\"\n",
    "     z_scores = np.abs((series - series.median()) / series.std())\n",
    "     outliers = z_scores > threshold\n",
    "     return (outliers.sum() / len(series)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiple_boxplots(data_frame, columns_for_boxplot, titles=None, num_boxplots_per_row=2):\n",
    "    # Calcular a quantidade;\n",
    "    num_boxplots = len(columns_for_boxplot)\n",
    "    num_rows = (num_boxplots + num_boxplots_per_row - 1) // num_boxplots_per_row\n",
    "\n",
    "    # Criar os subplots\n",
    "    fig = make_subplots(rows=num_rows, cols=num_boxplots_per_row, subplot_titles=titles)\n",
    "\n",
    "    # Loop para ir montando todos os gráficos em boxplot\n",
    "    for idx, column in enumerate(columns_for_boxplot):\n",
    "        row_idx = idx // num_boxplots_per_row + 1\n",
    "        col_idx = idx % num_boxplots_per_row + 1\n",
    "\n",
    "        data = data_frame[column]\n",
    "        box = go.Box(y=data, name=column)\n",
    "\n",
    "        fig.add_trace(box, row=row_idx, col=col_idx)\n",
    "\n",
    "    # Ajustando a forma\n",
    "    fig.update_layout(height=300*num_rows, showlegend=False)\n",
    "\n",
    "    # Plotar os gráficos\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_distribuicoes(data_frame, columns_for_distribution, num_distributions_per_row=2):\n",
    "    # Calcular a quantidade\n",
    "    num_distributions = len(columns_for_distribution)\n",
    "    num_rows = ceil(num_distributions / num_distributions_per_row)\n",
    "\n",
    "    # Criar os subplots\n",
    "    fig = make_subplots(rows=num_rows, cols=num_distributions_per_row)\n",
    "\n",
    "    # Loop para ir montando todos os gráficos de distribuição\n",
    "    for idx, column in enumerate(columns_for_distribution):\n",
    "        dados = data_frame[column].dropna()  # Remover valores ausentes\n",
    "\n",
    "        # Criar o gráfico de histograma\n",
    "        histogram_data = go.Histogram(x=dados, nbinsx=30, name=f'Histograma - {column}')\n",
    "\n",
    "        # Adicionar ao subplot\n",
    "        fig.add_trace(histogram_data,\n",
    "                      row=(idx // num_distributions_per_row) + 1, col=(idx % num_distributions_per_row) + 1)\n",
    "\n",
    "    # Atualizar o layout com títulos e legendas adequadas\n",
    "    for idx, column in enumerate(columns_for_distribution):\n",
    "        row_idx = (idx // num_distributions_per_row) + 1\n",
    "        col_idx = (idx % num_distributions_per_row) + 1\n",
    "\n",
    "        # Adicionar título ao subplot\n",
    "        fig.update_xaxes(title_text=f'{column}', row=row_idx, col=col_idx)\n",
    "        fig.update_yaxes(title_text='Quantidade', row=row_idx, col=col_idx)  # Adicionar título ao eixo Y\n",
    "\n",
    "    # Ajustando a forma\n",
    "    fig.update_layout(height=300*num_rows, showlegend=False)  # Remover a legenda\n",
    "\n",
    "    # Plotar os gráficos\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_fluxograma(df, origem, destino, intermediario, titulo, tam_fonte):\n",
    "    df_temp1 = df.groupby([origem, intermediario])['order_status'].count().reset_index()\n",
    "    df_temp1.columns = ['source', 'target', 'value']\n",
    "\n",
    "    df_temp2 = df.groupby([intermediario, destino])['delivery_status'].count().reset_index()\n",
    "    df_temp2.columns = ['source', 'target', 'value']\n",
    "\n",
    "    if destino == 'order_status':\n",
    "        df_temp2['target'] = df_temp2['target'].map({'CANCELED': 'Cancelado', 'FINISHED': 'Finalizado'})\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    links = pd.concat([df_temp1, df_temp2], axis=0)\n",
    "\n",
    "    unique_source_target = list(pd.unique(links[['source', 'target']].values.ravel('K')))\n",
    "\n",
    "    mapping_dict = {k: v for v, k in enumerate(unique_source_target)}\n",
    "\n",
    "    links['source'] = links['source'].map(mapping_dict)\n",
    "    links['target'] = links['target'].map(mapping_dict)\n",
    "\n",
    "    links_dict = links.to_dict(orient='list')\n",
    "\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=dict(\n",
    "            pad=15,\n",
    "            thickness = 20,\n",
    "            line = dict(color = 'black', width = 0.5),\n",
    "            label = unique_source_target,\n",
    "            color = 'blue'\n",
    "        ),\n",
    "        link = dict(\n",
    "            source = links_dict['source'],\n",
    "            target = links_dict['target'],\n",
    "            value = links_dict['value']\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    return fig.update_layout(title_text = titulo, font_size=tam_fonte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_multiple_col(col_names_list, df): \n",
    "    '''\n",
    "    AIM    -> Drop multiple columns based on their column names \n",
    "    \n",
    "    INPUT  -> List of column names, df\n",
    "    \n",
    "    OUTPUT -> updated df with dropped columns \n",
    "    ------\n",
    "    '''\n",
    "    df.drop(col_names_list, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlated_columns(df, interval):\n",
    "    \"\"\"\n",
    "    Encontra e exibe as correlações entre colunas de um DataFrame.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df: DataFrame pandas\n",
    "    - intervalo de correlação desejado (uma tupla de dois valores)\n",
    "\n",
    "    Retorna:\n",
    "    - Lista de tuplas representando pares de colunas correlacionadas.\n",
    "    \"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "    correlated_columns = []\n",
    "\n",
    "    # Iterar sobre as combinações de colunas para encontrar correlações\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            if interval[0] <= abs(corr) <= interval[1]:\n",
    "                col1 = correlation_matrix.columns[i]\n",
    "                col2 = correlation_matrix.columns[j]\n",
    "                correlated_columns.append((col1, col2))\n",
    "                print(f\"Correlação entre {col1} e {col2}: {corr}\")\n",
    "\n",
    "    # Plotar um mapa de calor da matriz de correlação\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='cubehelix_r')\n",
    "    plt.title('Matriz de Correlação')\n",
    "    plt.xlabel('Variáveis')\n",
    "    plt.ylabel('Variáveis')\n",
    "    plt.show()\n",
    "\n",
    "    return correlated_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlacao_com_variavel_alvo(df, target_variable, nivel=\"forte\", top_n=5):\n",
    "    \"\"\"\n",
    "    Imprime as n features com as maiores correlações com uma variável alvo, com base no nível escolhido.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df: DataFrame pandas.\n",
    "    - target_variable: String, nome da variável alvo.\n",
    "    - nivel: String que define o critério de correlação (\"forte\", \"fraca\", etc.).\n",
    "    - top_n: Número inteiro, quantidade de features a serem impressas.\n",
    "\n",
    "    Retorna:\n",
    "    - Nenhum (imprime as correlações).\n",
    "    \"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "    # Filtra as correlações com base no nível escolhido\n",
    "    if nivel.lower() == \"forte\":\n",
    "        filtered_correlations = correlation_matrix[((correlation_matrix >= 0.7) & (correlation_matrix < 1.0)) | ((correlation_matrix <= -0.7) & (correlation_matrix > -1.0))]\n",
    "    else:\n",
    "        raise ValueError(\"Nível não suportado. Atualmente, apenas 'forte' é suportado.\")\n",
    "\n",
    "    # Filtra as correlações com a variável alvo\n",
    "    correlations_with_target = filtered_correlations[target_variable].sort_values(ascending=False)\n",
    "\n",
    "    # Pegar as n maiores correlações\n",
    "    top_n_correlations = correlations_with_target.head(top_n)\n",
    "\n",
    "    # Imprimir as n maiores correlações com a variável alvo\n",
    "    print(f\"As {top_n} maiores correlações com '{target_variable}' ({nivel}):\")\n",
    "    for feature, correlation in top_n_correlations.items():\n",
    "        print(f\"{feature}: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format_to_datetime(dataframe, columns):\n",
    "    \"\"\"\n",
    "    Convert specified columns in a DataFrame from 'DD/MM/YYYY HH:MM:SS PM' to 'YYYY-MM-DD HH:MM:SS' format.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): The DataFrame to modify.\n",
    "    - columns (list): List of column names to convert.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        if column in dataframe.columns:\n",
    "            # Convert the specified column to datetime with the original format\n",
    "            dataframe[column] = pd.to_datetime(dataframe[column], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n",
    "        else:\n",
    "            print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avalia_modelo(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = round(np.sqrt(mse),2)\n",
    "    mae = round(mean_absolute_error(y_test,y_pred),2)\n",
    "    \n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos a junção de todas as tabelas em uma só para facilitar a manipulação conjunta dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando as bases\n",
    "channels = pd.read_csv('channels.csv')\n",
    "deliveries = pd.read_csv('deliveries.csv')\n",
    "drivers = pd.read_csv('drivers.csv')\n",
    "hubs = pd.read_csv('hubs.csv')\n",
    "orders = pd.read_csv('orders.csv')\n",
    "payments = pd.read_csv('payments.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "\n",
    "# Fazendo a unificação\n",
    "deliveries = pd.merge(left=drivers, right=deliveries, on='driver_id', how ='right')\n",
    "stores = pd.merge(left=hubs, right=stores, on='hub_id', how ='right')\n",
    "df = pd.merge(left=channels, right=orders, on='channel_id', how ='right')\n",
    "df = pd.merge(left=payments, right=df, on='payment_order_id', how ='right')\n",
    "df = pd.merge(left=deliveries, right=df, on='delivery_order_id', how ='right')\n",
    "df = pd.merge(left=stores, right=df, on='store_id', how ='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moment_order_columns = ['order_moment_created', 'order_moment_accepted','order_moment_ready', \n",
    "                        'order_moment_collected','order_moment_in_expedition', 'order_moment_delivering',\n",
    "                        'order_moment_finished']\n",
    "\n",
    "# Convertendo as colunas selecionadas para tipo datetime\n",
    "df = convert_format_to_datetime(df, columns=moment_order_columns)\n",
    "\n",
    "# Criando uma coluna com o tempo de entrega do pedido (essa é a variável que vai prevista) - se assemelha MUITO com order_metric_cycle_time\n",
    "df['tempo_entrega'] = (df['order_moment_finished'] - df['order_moment_accepted']).dt.round('MIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma coluna de data com base nas colunas existentes\n",
    "df.rename(columns = {'order_created_year':'year','order_created_month':'month','order_created_day':'day', 'order_created_hour': 'hour', 'order_created_minute': 'minute'}, inplace=True)\n",
    "df['order_date'] = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df = find_missing_percent(df)\n",
    "display(miss_df[miss_df['PercentMissing']>0.0])\n",
    "print(\"\\n\")\n",
    "print(f\"Número de colunas com valores faltantes:{str(miss_df[miss_df['PercentMissing']>0.0].shape[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tirando as colunas com mais de 70% de dados faltantes\n",
    "for coluna in miss_df.loc[miss_df['PercentMissing'] > 70]['ColumnName']:\n",
    "    df.drop(columns=coluna, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com base na análise da funcionalidade de cada coluna e na porcentagem de dados faltantes, vamos focar somente nas que podem nos trazer mais resultados e excluir as mais problemáticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observação: A coluna criada 'tempo_entrega' que utiliza o 'order_moment_accepted' e 'order_moment_finished' apresenta MUITA SEMELHANÇA com a coluna 'order_metric_cycle_time'.\n",
    "\n",
    "Quando os valores são muito diferentes, isso se dá porque o 'order_moment_finished', que corresponde ao momento que o estabelecimento finalizou o pedido, está muito depois do tempo correspondente à entrega em si, o que denota uma inconsistência nos dados\n",
    "\n",
    "Devido à isso, a coluna 'tempo_entrega' será desconsiderada para futuras análises e a métrica utilizada pra fazer as previsões em etapas posteriores será 'order_metric_cycle_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_descart = ['hub_id', 'hub_latitude', 'hub_longitude', 'store_id', 'store_latitude', 'store_longitude', 'driver_id',\n",
    "                   'delivery_id', 'delivery_order_id','payment_id', 'payment_order_id', 'channel_id', 'order_id',\n",
    "                   'hour', 'minute', 'day', 'month', 'year', 'tempo_entrega']\n",
    "\n",
    "df = drop_multiple_col(colunas_descart, df)\n",
    "\n",
    "'''Separando dados numéricos e categóricos '''\n",
    "numeric_cols = df.select_dtypes(['float','int']).columns\n",
    "categoric_cols = df.select_dtypes('object').columns\n",
    "\n",
    "df_numeric = df[numeric_cols]\n",
    "df_categoric = df[categoric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df = find_missing_percent(df)\n",
    "'''Displays columns with missing values'''\n",
    "display(miss_df[miss_df['PercentMissing']>0.0])\n",
    "print(\"\\n\")\n",
    "print(f\"Número de colunas com valores faltantes:{str(miss_df[miss_df['PercentMissing']>0.0].shape[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(['float','int']).columns\n",
    "for feature in numeric_cols:\n",
    "    qtd_outliers = calculate_outlier_percentage(df[feature]).round(2)\n",
    "    print(f'{qtd_outliers} % | {feature}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazendo boxplots, para visualizar os outliers\n",
    "create_multiple_boxplots(df, df_numeric.columns,num_boxplots_per_row=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotando distribuições\n",
    "plotar_distribuicoes(df, df_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_antes = df.shape[0]\n",
    "\n",
    "# removendo outliers de delivery_distance_meters\n",
    "outliers = df[df['delivery_distance_meters'] > 10000].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de payment_amount\n",
    "outliers = df[df['payment_amount'] > 300].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de payment_fee\n",
    "outliers = df[df['payment_fee'] > 10].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_amount\n",
    "outliers = df[df['order_amount'] > 300].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_delivery_fee\n",
    "outliers = df[df['order_delivery_fee'] > 27].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_delivery_cost\n",
    "outliers = df[df['order_delivery_cost'] > 15].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_collected_time\n",
    "outliers = df[(df['order_metric_collected_time'] > 7) | (df['order_metric_walking_time'] < 0)].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_paused_time\n",
    "outliers = df[ (df['order_metric_paused_time'] < 0) | (df['order_metric_paused_time'] > 20)].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_production_time\n",
    "outliers = df[df['order_metric_production_time'] > 45].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_walking_time\n",
    "outliers = df[(df['order_metric_walking_time'] > 10) | (df['order_metric_walking_time'] < 0)].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_expediton_speed_time\n",
    "outliers = df[df['order_metric_expediton_speed_time'] > 20].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_transit_time\n",
    "outliers = df[df['order_metric_transit_time'] > 45].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de order_metric_cycle_time\n",
    "outliers = df[df['order_metric_cycle_time'] > 90].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# removendo outliers de tempo_entrega que excedam 1 dia para realizar a entrega\n",
    "#outliers = df[df['tempo_entrega'] > pd.Timedelta(days=1)].index\n",
    "#df.drop(outliers, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "linhas_depois = df.shape[0]\n",
    "restante = round((100* linhas_depois / linhas_antes), 2)\n",
    "print(f'total de linhas retiradas: {linhas_antes - linhas_depois} (restam {linhas_depois} linhas que equivalem a {restante}% da base inicial)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['payment_status'].eq('AWAITING')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo AWAITING de 'paymen_status' porque são somente 10 valores e 8 deles não apresentam a métrica de interesse \n",
    "outliers = df[df['payment_status'].eq('AWAITING')].index\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar distribuições\n",
    "plotar_distribuicoes(df, df_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df.describe().T,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "df_date = df.copy()\n",
    "\n",
    "# Criando colunas para facilitar análises\n",
    "df_date['minute'] = df['order_date'].dt.minute\n",
    "df_date['hour'] = df['order_date'].dt.hour\n",
    "df_date['day'] = df['order_date'].dt.day\n",
    "df_date['month'] = df['order_date'].dt.month\n",
    "df_date['year'] = df['order_date'].dt.year\n",
    "df_date['weekday'] = df['order_date'].dt.weekday # Domingo é 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupando o DataFrame pelas cidades e seus estados correspondentes\n",
    "grouped_data = df.groupby(['hub_city', 'hub_state']).size().reset_index(name='Count')\n",
    "\n",
    "# Criando o treemap\n",
    "fig = px.treemap(grouped_data, path=['hub_state', 'hub_city'], values='Count',\n",
    "                 title='Cidades em que Hack Eats está presente')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cria_fluxograma(df, origem='hub_name', destino='order_status', intermediario='hub_state', \n",
    "                titulo='Fluxo Cancelamento das Distribuições por Estados', tam_fonte=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cria_fluxograma(df_date, origem='hub_state', destino='order_status', intermediario='hour', \n",
    "                titulo='Fluxo Cancelamento dos Estados por Hora do Dia', tam_fonte=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp1 = df.groupby(['hub_state', 'driver_modal'])['order_status'].count().reset_index()\n",
    "df_temp1.columns = ['source', 'target', 'value']\n",
    "df_temp1['target'] = df_temp1['target'].map({'BIKER': 'Ciclista', 'MOTOBOY': 'Motociclista'})\n",
    "\n",
    "df_temp2 = df.groupby(['driver_modal', 'order_status'])['delivery_status'].count().reset_index()\n",
    "df_temp2.columns = ['source', 'target', 'value']\n",
    "df_temp2['source'] = df_temp2['source'].map({'BIKER': 'Ciclista', 'MOTOBOY': 'Motociclista'})\n",
    "df_temp2['target'] = df_temp2['target'].map({'CANCELED': 'Cancelado', 'FINISHED': 'Finalizado'})\n",
    "\n",
    "links = pd.concat([df_temp1, df_temp2], axis=0)\n",
    "\n",
    "unique_source_target = list(pd.unique(links[['source', 'target']].values.ravel('K')))\n",
    "\n",
    "mapping_dict = {k: v for v, k in enumerate(unique_source_target)}\n",
    "\n",
    "links['source'] = links['source'].map(mapping_dict)\n",
    "links['target'] = links['target'].map(mapping_dict)\n",
    "\n",
    "links_dict = links.to_dict(orient='list')\n",
    "\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness = 20,\n",
    "        line = dict(color = 'black', width = 0.5),\n",
    "        label = unique_source_target,\n",
    "        color = 'blue'\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = links_dict['source'],\n",
    "        target = links_dict['target'],\n",
    "        value = links_dict['value']\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text = 'Fluxo Cancelamento dos Estados por Tipo de Entregador', font_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = 'hour'\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(x=x_plot, data=df_date, palette='viridis', hue='order_status')\n",
    "plt.xlabel('Horário do Dia')\n",
    "plt.ylabel('Pedidos')\n",
    "plt.title('Quantidade de Pedidos por Horário do Dia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = 'weekday'\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(x=x_plot, data=df_date[~df_date['delivery_status'].eq('DELIVERING')], hue='delivery_status')\n",
    "plt.xlabel('Dia da Semana (Início no Domingo)')\n",
    "plt.ylabel('Pedidos')\n",
    "plt.title('Pedidos por Dias da Semana')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = 'weekday'\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "#sns.countplot(x=x_plot, data=df, hue=x_plot, legend=False, palette='viridis')\n",
    "sns.countplot(x=x_plot, data=df_date, hue='hub_state')\n",
    "plt.xlabel('Dia da Semana (Início no Domingo)')\n",
    "plt.ylabel('Pedidos')\n",
    "plt.title('Quantidade de Pedidos por Dia da Semana em Diferentes Estados')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = 'month'\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(x=x_plot, data=df_date)\n",
    "plt.xlabel('Mês do Ano')\n",
    "plt.ylabel('Pedidos')\n",
    "plt.title('Quantidade de Pedidos por Mês do Ano')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_plot = 'order_metric_cycle_time'\n",
    "x_plot = 'delivery_distance_meters'\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=x_plot,y=y_plot, data=df)\n",
    "plt.xlabel('Tempo de Entrega')\n",
    "plt.ylabel('Distância de Entrega')\n",
    "plt.title('Distância por Tempo de Entrega')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='driver_type', y='order_metric_cycle_time', histfunc='avg')\n",
    "fig.update_layout(\n",
    "    title='Tempo médio de entrega por tipo de motorista',\n",
    "    xaxis_title='Tipo de motorista',\n",
    "    yaxis_title='Tempo médio de entrega'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='store_name', y='order_metric_cycle_time', histfunc='avg')\n",
    "fig.update_layout(\n",
    "    title='Tempo médio de entrega por loja',\n",
    "    xaxis_title='Loja',\n",
    "    yaxis_title='Tempo médio de entrega'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='payment_method', y='order_metric_cycle_time', histfunc='avg')\n",
    "fig.update_layout(\n",
    "    title='Tempo médio de entrega por método de pagamento',\n",
    "    xaxis_title='Método de pagamento',\n",
    "    yaxis_title='Tempo médio de entrega'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='channel_name', y='order_metric_cycle_time', histfunc='avg')\n",
    "fig.update_layout(\n",
    "    title='Tempo médio de entrega por canal',\n",
    "    xaxis_title='Canais',\n",
    "    yaxis_title='Tempo médio de entrega'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_plot = 'order_metric_cycle_time'\n",
    "x_plot = 'weekday'\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=x_plot,y=y_plot, data=df_date)\n",
    "plt.xlabel('Dia da Semana')\n",
    "plt.ylabel('Tempo Médio de Entrega')\n",
    "plt.title('Tempo Médio de Entrega por Dia da Semana')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_plot = 'order_metric_cycle_time'\n",
    "x_plot = 'hour'\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=x_plot,y=y_plot, data=df_date)\n",
    "plt.xlabel('Horário do dia')\n",
    "plt.ylabel('Tempo Médio de Entrega')\n",
    "plt.title('Tempo Médio de Entrega por Horário do dia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = 'order_metric_cycle_time'\n",
    "y_plot = 'hub_state'\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=x_plot,y=y_plot, data=df_date, orient='h')\n",
    "plt.ylabel('Estado')\n",
    "plt.xlabel('Tempo Médio de Entrega')\n",
    "plt.title('Tempo Médio de Entrega por Estado')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = 'order_metric_cycle_time'\n",
    "y_plot = 'hub_name'\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=x_plot,y=y_plot, data=df_date, orient='h')\n",
    "plt.ylabel('Estado')\n",
    "plt.xlabel('Tempo Médio de Entrega')\n",
    "plt.title('Tempo Médio de Entrega por Local de Distribuição')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_plot = 'order_metric_cycle_time'\n",
    "x_plot = 'driver_modal'\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=x_plot,y=y_plot, data=df_date)\n",
    "plt.xlabel('DRIVER_MODAL')\n",
    "plt.ylabel('Tempo Médio de Entrega')\n",
    "plt.title('Tempo Médio de Entrega por DRIVER_MODAL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = 'order_metric_cycle_time'\n",
    "y_plot = 'channel_type'\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=x_plot,y=y_plot, data=df_date, orient='h')\n",
    "plt.ylabel('Tipo de Canal')\n",
    "plt.xlabel('Tempo Médio de Entrega')\n",
    "plt.title('Tempo Médio de Entrega por Tipo de Canal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = 'order_metric_cycle_time'\n",
    "y_plot = 'channel_type'\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=x_plot,y=y_plot, data=df_date, orient='h', hue='hub_state')\n",
    "plt.ylabel('Tipo de Canal')\n",
    "plt.xlabel('Tempo Médio de Entrega')\n",
    "plt.title('Tempo Médio de Entrega por Tipo de Canal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os valores outliers já foram tratados na etapa de limpeza dos dados, agora cabe lidar com os valores nulos e com o tratamento das colunas para viabilizar a utilização dos modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mantendo somente os pedidos que não foram cancelados para que possamos analisar os tempos de entrega\n",
    "df = df[~df['order_status'].eq('CANCELED')]\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "df['order_moment_accepted'] = pd.to_datetime(df['order_moment_accepted'])\n",
    "\n",
    "# Criando colunas para facilitar análises\n",
    "# Colunas para o tempo de realização do pedido\n",
    "df['minute_rea'] = df['order_date'].dt.minute\n",
    "df['hour_rea'] = df['order_date'].dt.hour\n",
    "df['day_rea'] = df['order_date'].dt.day\n",
    "df['month_rea'] = df['order_date'].dt.month\n",
    "df['year_rea'] = df['order_date'].dt.year\n",
    "df['weekday_rea'] = df['order_date'].dt.weekday # Domingo é 0\n",
    "\n",
    "# Colunas para o tempo de aceitação do pedido\n",
    "df['minute_ac'] = df['order_moment_accepted'].dt.minute\n",
    "df['hour_ac'] = df['order_moment_accepted'].dt.hour\n",
    "df['day_ac'] = df['order_moment_accepted'].dt.day\n",
    "df['month_ac'] = df['order_moment_accepted'].dt.month\n",
    "df['year_ac'] = df['order_moment_accepted'].dt.year\n",
    "df['weekday_ac'] = df['order_moment_accepted'].dt.weekday # Domingo é 0\n",
    "\n",
    "# Tempo para aceitar o pedido\n",
    "df['dif_tempo'] = df['order_moment_accepted'] - df['order_date']\n",
    "df['min_para_aceitar'] = df['dif_tempo'].apply(lambda x: round(x.total_seconds() / 60 ,2))\n",
    "\n",
    "df.drop(columns=['order_date', 'order_moment_accepted', 'dif_tempo'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo colunas que não serão úteis para prever o tempo de entrega, já que não teremos informações delas no momento do pedido\n",
    "col_del = ['order_status', 'payment_status', 'delivery_status', 'order_moment_ready', 'order_moment_collected',\n",
    "       'order_moment_in_expedition', 'order_moment_delivering',\n",
    "       'order_moment_finished', 'order_metric_collected_time',\n",
    "       'order_metric_paused_time', 'order_metric_production_time',\n",
    "       'order_metric_walking_time', 'order_metric_expediton_speed_time',\n",
    "       'order_metric_transit_time']\n",
    "\n",
    "# tirando os ano já que só possuo dados de 2021\n",
    "col_del = col_del + ['year_rea', 'year_ac'] \n",
    "df.drop(col_del, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_missing_percent(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_antes = df.shape[0]\n",
    "# Removendo pedidos sem o tempo de entrega 'order_metric_cycle_time'\n",
    "df = df[~df['order_metric_cycle_time'].isnull()]\n",
    "\n",
    "# Preenchendo valores faltantes do store_plan_price para poder dividir em colunas por OneHotEncoding\n",
    "df['store_plan_price'].fillna(-1, inplace=True)\n",
    "\n",
    "# REMOVENDO TODAS AS LINHAS COM VALORES NULOS \n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "linhas_depois = df.shape[0]\n",
    "print(f'apaguei {linhas_antes - linhas_depois} linhas e restam {round(100*linhas_depois / linhas_antes)}% do df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos listar as features que vamos utilizar\n",
    "input_cols_categoric = ['hub_state','channel_type', 'store_plan_price']\n",
    "input_cols_numeric = ['delivery_distance_meters', 'payment_amount', 'payment_fee', 'order_amount',  \n",
    "                    'minute_rea', 'hour_rea', 'day_rea', 'month_rea',\n",
    "                     'weekday_rea', 'minute_ac', 'hour_ac','day_ac', 'month_ac', \n",
    "                     'weekday_ac', 'min_para_aceitar']\n",
    "\n",
    "features = input_cols_categoric + input_cols_numeric\n",
    "\n",
    "target = 'order_metric_cycle_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo os dados \n",
    "X = df[features]  \n",
    "y = df[target]  \n",
    "\n",
    "# Divide em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=999\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando as variáveis categóricas \n",
    "\n",
    "encoder = ce.OneHotEncoder(cols=input_cols_categoric, handle_unknown='ignore')\n",
    "\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros do modelo com melhor previsão do tempo de entrega\n",
    "parametros= {'base_score': None,\n",
    "            'booster': 'gbtree',\n",
    "            'callbacks': None,\n",
    "            'colsample_bylevel': None,\n",
    "            'colsample_bynode': None,\n",
    "            'colsample_bytree': None,\n",
    "            'device': None,\n",
    "            'early_stopping_rounds': None,\n",
    "            'enable_categorical': False,\n",
    "            'eval_metric': None,\n",
    "            'feature_types': None,\n",
    "            'gamma': 0.3,\n",
    "            'grow_policy': None,\n",
    "            'importance_type': None,\n",
    "            'interaction_constraints': None,\n",
    "            'learning_rate': 0.05,\n",
    "            'max_bin': None,\n",
    "            'max_cat_threshold': None,\n",
    "            'max_cat_to_onehot': None,\n",
    "            'max_delta_step': None,\n",
    "            'max_depth': 12,\n",
    "            'max_leaves': None,\n",
    "            'min_child_weight': None,\n",
    "            'monotone_constraints': None,\n",
    "            'multi_strategy': None,\n",
    "            'n_estimators': 900,\n",
    "            'n_jobs': None,\n",
    "            'num_parallel_tree': None,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'random_state': 0,\n",
    "            'reg_alpha': 0.4,\n",
    "            'reg_lambda': 0.1,\n",
    "            'sampling_method': None,\n",
    "            'scale_pos_weight': None,\n",
    "            'subsample': 0.9,\n",
    "            'tree_method': None,\n",
    "            'validate_parameters': None,\n",
    "            'verbosity': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo com os parâmetros\n",
    "XGB_model = XGBRegressor(**parametros)\n",
    "\n",
    "# Fazendo uma validação cruzada para verificar os resultados no dados de treino\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "result = cross_val_score(XGB_model, X_train, y_train, cv = kfold, scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados da validação cruzada\n",
    "print(f'K-Fold RMSE Scores: {result}')\n",
    "print(f'Mean RMSE for Cross-Validation K-Fold: {result.mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando o modelo com os dados de teste, de forma avaliar a eficiência dele em dados novos (assim como em uma situação real)\n",
    "XGB_model.fit(X_train, y_train)\n",
    "rmse, mae = avalia_modelo(XGB_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'RMSE: {rmse}  |  MAE: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importância de cada coluna no modelo\n",
    "feature_imp = pd.Series(XGB_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "# Criando um gráfico de barras\n",
    "sns.set_palette(\"mako_r\")\n",
    "_ = plt.figure(figsize=(10, 6))\n",
    "_ = sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "_ = plt.xlabel(\"Feature Importance Score\")\n",
    "_ = plt.ylabel(\"Features\")\n",
    "_ = plt.title(\"Visualizing Important Features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
